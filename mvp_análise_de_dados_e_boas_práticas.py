# -*- coding: utf-8 -*-
"""MVP - Análise de Dados e Boas Práticas.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uvKgvtqyP2HwEFXbEc_07sS4uq2AN_ad

# MVP Análise de Dados e Boas Práticas

**Nome:** Tiago Leite Emiliano

**Matrícula:** 111.704.796-26

**Dataset:** Coffe Sales (https://www.kaggle.com/datasets/ihelon/coffee-sales)

# Descrição do Problema
Este conjunto de dados contém registros detalhados de vendas de café de uma máquina de venda automática.

A base de dados se destina à análise de padrões de compra dos clientes, preferências do cliente relacionadas a produtos e faturamento médio.

O conjunto de dados abrange de março de 2024 até fevereiro de 2025, capturando dados de transações diárias realizadas nesse ano corrente.

## Hipóteses do Problema

As hipóteses que tracei são as seguintes:

- Quais são os tipos de café mais vendidos pela máquina?
- Existe alguma faixa de horário de preferencia dos clientes?
- Qual é a forma de pagamento de preferencia dos clientes?
- Existe alguma relação entre tipos de produto vendidos e horário de compra?
- Quais são as horas do dia que mais se vende?
- Considerando que um cliente fidelizado comprou em média 20 vezes (aprox. 2 vezes no mês) no último ano, conseguimos determinar quem são e a quantidade de pessoas nessa categoria?

## Tipo de Problema

Este é um problema de **classificação supervisionada.** Dado que temos a frequencia de compra dos clientes, queremos determinar quem é fidelizado ou cliente novo.

## Seleção de Dados

O dataset Coffee Sales é recomendado no Kaggle por completar todos os requisitos de usabilidade da plataforma e ter nota máxima nesse sentido, o que garante uma uniformidade dos dados a serem utilizados. Fiz algumas analises explortórias iniciais na base para entender se precisava de alguma tratativa de limpeza, e não observei essa necessidade.

## Atributos do Dataset


O dataset contem 3263 linhas de transações realizadas com 6 atributos:
- ***date*** (data da compra)
- ***datetime*** (data e hora da compra)
- ***cash_type*** (tipo de pagamento realizado)
- ***card*** (numero do cartão utilizada)
- ***money*** (valor pago)
- ***coffee_name*** (tipo de produto vendido)

# Importação das Bibliotecas Necessárias e Carga de Dados

Esta seção consolida todas as importações de bibliotecas necessárias para a análise, visualização e pré-processamento dos dados, bem como o carregamento inicial do dataset Iris.
"""

# Configuração para não exibir os warnings
#import warnings
#warnings.filterwarnings("ignore")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
! pip install https://github.com/pandas-profiling/pandas-profiling/archive/master.zip
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder
from sklearn.decomposition import PCA
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# Informa a URL de importação do dataset
url = "https://raw.githubusercontent.com/emilianotiago/dados_coffee-sales/refs/heads/main/index_1.csv"

# Lê o arquivo
dataset = pd.read_csv(url, delimiter=',')

# Verificando os tipos de cada coluna
dataset.dtypes

# Exibindo as primeiras linhas
dataset.head()

#Exibindo últimas linhas
dataset.tail()

#Nomes das colunas
dataset.columns

#Dimensão do dataset
dataset.shape

#Linhas por coluna
dataset.count()

"""# Análise de Dados

Nesta etapa de Análise de Dados Exploratória (EDA) sobre o dataset Coffee Sales, visamos entender a distribuição em vendas, tipos de produtos e tempo para termos uma modelagem adequada ao desafio do negócio.

## Total e Tipo das Instâncias

O dataset possui 3263 instâncias (observações), com 3174 instâncias definindo o tipo de pagamento card (cartão de crédito). As colunas date, datetime, cash_type, card e coffe_name são tipo categórico, enquanto money é numérico (float).
"""

print(f"Total de instâncias: {len(dataset)}")
print("\nTipos de dados por coluna:")
print(dataset.info())

plt.figure(figsize=(15, 7))

# Obter a ordem dos tipos de café pela contagem de vendas
order = dataset['coffee_name'].value_counts().index

# gráfico de barras simples, ordenado pela contagem de vendas
ax = sns.countplot(x='coffee_name', data=dataset, order=order)
plt.title('Distribuição (em vendas) dos tipos de café vendidos')
plt.xlabel('Vendas (em quantidade)')
plt.ylabel('Contagem')

# Adicionar rótulos de dados
for p in ax.patches:
    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='center', xytext=(0, 5), textcoords='offset points')

plt.show()

"""O gráfico de barras mostra que os produtos mais vendidos nesse último ano foi o Americano with Milk e Latte com mais de 700 vendas realizadas, seguidos por Americano e Capuccino, com cerca de 450 vendas cada produto.

## Estatísticas Descritivas

Estatísticas descritivas fornecem um resumo das características numéricas, incluindo média, desvio padrão, mínimo, máximo e quartis.
"""

# estatísticas descritivas básicas do dataset
display(dataset.describe())

"""### Utilizando a média para chegar no ticket médio do negócio

A média é uma medida de tendência central que representa o valor típico ou o ponto de equilíbrio de um conjunto de dados. É calculada somando-se todos os valores e dividindo-se pelo número total de observações. É sensível a valores extremos (outliers).
"""

# média do atributo numérico 'money' do dataset
mean_money = dataset['money'].mean()
print(f"O ticket médio do valor das vendas é de: U${mean_money:.2f}")

"""### Tipo de pagamento mais comum (Proporção)"""

# Contar a frequência de cada tipo de pagamento
payment_type_counts = dataset['cash_type'].value_counts().reset_index()

# Renomear as colunas para o formato desejado
payment_type_counts.columns = ['Tipo de Pagamento', 'Total de vendas por tipo de pagamento']

# Exibir o DataFrame resultante sem o índice
display(payment_type_counts.style.hide(axis="index"))

# Contar a frequência de cada tipo de pagamento
payment_counts = dataset['cash_type'].value_counts()

# Criar o gráfico de pizza
plt.figure(figsize=(4, 4))
plt.pie(payment_counts, labels=payment_counts.index, autopct='%1.1f%%', startangle=140, colors=['skyblue', 'lightcoral'])
plt.title('Distribuição de Vendas por Tipo de Pagamento')
plt.axis('equal') # Garante que o gráfico de pizza seja um círculo.
plt.show()

"""### Desvio Padrão

O desvio padrão é uma medida de dispersão que quantifica a quantidade de variação ou dispersão de um conjunto de valores. Um desvio padrão baixo indica que os pontos de dados tendem a estar próximos da média do conjunto, enquanto um desvio padrão alto indica que os pontos de dados estão espalhados por uma faixa maior de valores. Ele é a raiz quadrada da variância.
"""

# desvio padrão do atributo numérico 'money' do dataset
std_money = dataset['money'].std()
print(f"O desvio padrão do valor das vendas é de {std_money:.2f}")

# Calcular o coeficiente de variação
coefficient_of_variation = (std_money / mean_money) * 100

# Exibir o coeficiente de variação formatado para duas casas decimais
print(f"O coeficiente de variação do valor das vendas é de {coefficient_of_variation:.2f}%.")

# Calcular a quantidade vendida de cada tipo de café
coffee_counts = dataset['coffee_name'].value_counts().reset_index()
coffee_counts.columns = ['coffee_name', 'quantidade_vendida']

# Calcular a média do valor das vendas por tipo de café (já fizemos isso, mas vamos garantir que temos)
mean_money_by_coffee = dataset.groupby('coffee_name')['money'].mean().reset_index()
mean_money_by_coffee.columns = ['coffee_name', 'media_dinheiro']


# Combinar as duas informações em um único DataFrame
coffee_summary = pd.merge(mean_money_by_coffee, coffee_counts, on='coffee_name')

# Calcular o total geral de vendas
total_vendas = coffee_summary['quantidade_vendida'].sum()

# Calcular a porcentagem de vendas para cada tipo de café
coffee_summary['percentual_vendas (%)'] = (coffee_summary['quantidade_vendida'] / total_vendas) * 100

# Ordenar a tabela pela quantidade vendida em ordem decrescente
coffee_summary_sorted = coffee_summary.sort_values(by='quantidade_vendida', ascending=False).reset_index(drop=True) # Reset index after sorting

# Calcular o percentual acumulado
coffee_summary_sorted['percentual_acumulado (%)'] = coffee_summary_sorted['percentual_vendas (%)'].cumsum()


# Exibir o DataFrame resultante ordenado com o percentual
display(coffee_summary_sorted)

"""### Observações Gerais sobre as Estatísticas Descritivas

Com os dados acima, é possível chegar em algumas conclusões sobre os dados:
 - Estamos com a venda concentrada em 4 produtos, representando aproximadamente 73% das vendas realizadas pela máquina;
 - Com a média de valor das vendas em U$ 31,80 e um coeficiente de varição em 15,5%, temos uma variação nas vendas considarada moderada, número impactado pela concentração grande em poucos produtos e até por uma variação pequena dos preços entre esses produtos com melhor performance;
 - Claramente o cliente prefere usar o cartão de crédito para pagamentos;
 - U\$ 40 parece realmente o limite pago pelos clientes no produtos. O que não indica que o cliente não possa pagar mais, mas parece um limite determinado pelo negócio.

## Histograma

A distribuição de dados descreve como os valores de uma variável se espalham, ou seja, a frequência com que diferentes valores ocorrem. Entender a distribuição é crucial na análise de dados, pois revela padrões, tendências centrais, dispersão e a presença de valores atípicos (outliers). O histograma é uma ferramenta visual fundamental para representar essa distribuição, mostrando a forma dos dados, se são simétricos ou assimétricos, unimodais ou multimodais.

### Como os preço dos produtos variaram  durante o período?
"""

# Converter a coluna 'datetime' para o tipo datetime, se ainda não estiver
dataset['datetime'] = pd.to_datetime(dataset['datetime'])

# Extrair a hora do dia da coluna 'datetime'
dataset['hour'] = dataset['datetime'].dt.hour

# Criar um histograma da distribuição das vendas por hora
plt.figure(figsize=(15, 6))
ax = sns.histplot(data=dataset, x='hour', bins=24, kde=False)
plt.title('Distribuição de Vendas por Hora do Dia')
plt.xlabel('Hora do Dia (0-23)')
plt.ylabel('Número de Vendas')
plt.xticks(range(0, 24)) # Garantir que todos os rótulos de hora sejam exibidos
plt.grid(False) # Remover as linhas de grade

# Adicionar rótulos de dados
for p in ax.patches:
    height = p.get_height()
    if height > 0:  # Apenas adicione rótulos se a altura for maior que 0
        ax.annotate(f'{int(height)}', (p.get_x() + p.get_width() / 2., height),
                    ha='center', va='center', xytext=(0, 5), textcoords='offset points')

plt.show()

"""O histograma mostra a distribuição das vendas em horas do dia mostra que as vendas começam as 6 da manhã e terminam as 22 horas.
As vendas tem um leve pico entre 10 e 11 da manhã, mas segue bem estável ao longo do dia, tendo uma queda após as 20 horas.

Temos claras paradas em vendas entre algumas horas, o que pode ser explicado por possíveis paradas para reposição de produtos.

## Boxplot

### Variação de preços praticados por produto

Olhando mais no detalhe os produtos vendidos, conseguimos determinar melhor como funciona a política de preços e suas variações.
"""

# Estatísticas descritivas do atributo numérico 'money' agrupadas por tipo de café
display(dataset.groupby('coffee_name')['money'].describe())

# Boxplot do valor monetário pago por tipo de produto
plt.figure(figsize=(13, 6))
sns.boxplot(x='coffee_name', y='money', data=dataset)
plt.title('Valor monetário pago por tipo de produto')
plt.xlabel('Produtos')
plt.ylabel('U\$\$') # Corrigido para escapar os símbolos de dólar
plt.xticks(rotation=45, ha='right') # Rotacionar rótulos do eixo X para melhor visualização
plt.grid(axis='y', linestyle='--', alpha=0.7) # Adicionar linhas de grade horizontais
plt.tight_layout() # Ajustar layout para evitar sobreposição de rótulos
plt.show()

"""O boxplot mostra que temos claras diferenças em políticas de preço por produto. Quando olhamos o Americano with Milk e Latte (carros-chefes em vendas) vemos medianas em patamares diferentes (U\$ 32 e U\$ 37, respectivamente) e como eles vendem em patamares parecidos, o Latte aparenta ser um produto mais interessante do negócio.

Destaca-se também itens como Hot Chocolate, Cocoa e Capuccino trabalhando em preços bem acima do preço médio do negócio, sendo entre eles o Cappuccino com melhor performance de vendas entre eles, o que mostra que sabor está determinando mais o perfil de compra entre produtos do que necessariamente o preço.

### Vendas de produtos por faixa de horário
Podemos também determinar a variação de vendas do produtos por hora. Como mostra o boxplot abaixo, existe uma concentração do consumo entrea aproximadamente 11 da manhã a 16 horas (olhando comportamento das medianas) sendo o Hot Chocolate sendo consumido mais tarde (prox. as 17 horas) e o Cortado, com o seu fluxo acontecendo mais cedo (prox. as 11 da manhã).

Isso pode determinar as necessidades de reposição evitando esse horário de fluxo e conseguindo antecipar essa demanda durante todo o dia. (gráfico abaixo)
"""

plt.figure(figsize=(15, 7))
sns.boxplot(x='coffee_name', y='hour', data=dataset)
plt.title('Distribuição das Horas de Venda por Tipo de Café')
plt.xlabel('Tipo de Café')
plt.ylabel('Hora do Dia (0-23)')
plt.xticks(rotation=45, ha='right')
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

"""## Clientes Fidelizados

Existem clientes fidelizados? Abaixo segue uma análise, olhando para a dispersão, de como os clientes se comportam, baseado na sua frequencia de compra.
"""

# Contar o número de transações por cliente (cartão)
customer_transaction_counts = dataset['card'].value_counts().reset_index()
customer_transaction_counts.columns = ['card', 'transaction_count']

# Exibir a distribuição das contagens de transação para ter uma ideia
print("Distribuição do número de transações por cliente:")
# Formatando a saída do describe para mostrar apenas as estatísticas e um rótulo customizado
desc_output = customer_transaction_counts['transaction_count'].describe()
print("Total de transações:")
display(desc_output) # A informação de dtype ainda pode aparecer dependendo da configuração de exibição do pandas/colab


# Definir o limite para clientes fidelizados (aproximadamente 30 transações)
loyal_customer_threshold = 20
tolerance = 5 # Permitir uma variação de +/- 5 transações em torno do limite

# Filtrar clientes com contagem de transações perto do limite
loyal_customers = customer_transaction_counts[
    (customer_transaction_counts['transaction_count'] >= loyal_customer_threshold - tolerance) &
    (customer_transaction_counts['transaction_count'] <= loyal_customer_threshold + tolerance)
]

# Ordenar a lista de clientes fidelizados pela contagem de transações
loyal_customers_sorted = loyal_customers.sort_values(by='transaction_count', ascending=False)

# Renomear a coluna 'card' para 'cliente' APENAS para exibição
loyal_customers_display = loyal_customers_sorted.rename(columns={'card': 'cliente'})

# Exibir os clientes potencialmente fidelizados com o nome da coluna alterado
print(f"\nClientes com aproximadamente {loyal_customer_threshold} transações (tolerância de +/- {tolerance}):")
display(loyal_customers_display)

# Opcional: Exibir o número total de clientes identificados
print(f"\nTotal de clientes identificados como potencialmente fidelizados: {len(loyal_customers_sorted)}")

"""### Analisando o comportamento desses clientes em gráfico de dispersão e boxplot"""

# Gráfico de dispersão da frequência de compra por cliente
plt.figure(figsize=(15, 7))
plt.scatter(x=customer_transaction_counts.index, y=customer_transaction_counts['transaction_count'], alpha=0.6)
plt.title('Gráfico de Dispersão da Frequência de Compra por Cliente')
plt.xlabel('Índice do Cliente') # O índice representa cada cliente individualmente
plt.ylabel('Número de Transações')
plt.grid(True, linestyle='--', alpha=0.6)
plt.yticks(range(0, int(customer_transaction_counts['transaction_count'].max()) + 20, 10)) # Ajustar eixo Y de 10 em 10
plt.show()

# 1. Calcular o ticket médio por cliente
average_spend_per_customer = dataset.groupby('card')['money'].mean().reset_index()
average_spend_per_customer.columns = ['card', 'average_spend']

# 2. Calcular o tempo desde a última compra por cliente
# Encontrar a data da última compra para cada cliente
last_purchase_time = dataset.groupby('card')['datetime'].max().reset_index()
last_purchase_time.columns = ['card', 'last_purchase_datetime']

# Definir um ponto de referência (a data mais recente no dataset)
reference_time = dataset['datetime'].max()

# Calcular a diferença de tempo entre o ponto de referência e a última compra de cada cliente
last_purchase_time['time_since_last_purchase'] = (reference_time - last_purchase_time['last_purchase_datetime']).dt.days # Diferença em dias

# Combinar todas as métricas de cliente (contagem de transações, ticket médio, tempo desde a última compra)
# Começamos com as contagens de transação (já calculadas em customer_transaction_counts)
customer_metrics = customer_transaction_counts.copy()

# Combinar com o ticket médio
customer_metrics = pd.merge(customer_metrics, average_spend_per_customer, on='card')

# Combinar com o tempo desde a última compra
customer_metrics = pd.merge(customer_metrics, last_purchase_time[['card', 'time_since_last_purchase']], on='card')

# Renomear as colunas do DataFrame customer_metrics para português
customer_metrics.rename(columns={
    'card': 'Cliente',
    'transaction_count': 'Compras',
    'average_spend': 'Ticket médio',
    'time_since_last_purchase': 'Tempo desde a última compra'
}, inplace=True)



#Exibir estatísticas descritivas das novas métricas
print("\nEstatísticas Descritivas das Métricas de Cliente:")
display(customer_metrics.describe())

# Filtrar clientes que fizeram mais de uma compra
multi_purchase_customers = customer_transaction_counts[customer_transaction_counts['transaction_count'] > 1]['card']

# Filtrar o dataset original para incluir apenas transações desses clientes
dataset_multi_purchase = dataset[dataset['card'].isin(multi_purchase_customers)].copy()

# Garantir que as transações estejam ordenadas por data/hora para cada cliente
dataset_multi_purchase.sort_values(by=['card', 'datetime'], inplace=True)

# Calcular a diferença de tempo entre transações consecutivas para cada cliente
# Usamos shift(-1) para subtrair a data/hora da compra atual da data/hora da PRÓXIMA compra
# ou shift(1) para subtrair a data/hora da compra ANTERIOR da data/hora da compra atual.
# A segunda opção (shift(1)) é mais comum para calcular o "tempo desde a última compra".
# Para o intervalo ENTRE compras, vamos usar a diferença da data atual para a data anterior.
dataset_multi_purchase['time_diff'] = dataset_multi_purchase.groupby('card')['datetime'].diff()

# Remover a primeira transação de cada cliente, pois a diferença de tempo será NaT (Not a Time)
dataset_multi_purchase.dropna(subset=['time_diff'], inplace=True)

# Calcular o intervalo médio de tempo entre as compras para cada cliente em dias
average_interval_per_customer = dataset_multi_purchase.groupby('card')['time_diff'].mean().dt.days.reset_index()
average_interval_per_customer.columns = ['card', 'Intervalo médio entre compras']

# Exibir estatísticas descritivas do intervalo médio
print("\nEstatísticas Descritivas do Intervalo Médio entre Compras:")
display(average_interval_per_customer.describe())

plt.figure(figsize=(8, 6))
sns.boxplot(y=average_interval_per_customer['Intervalo médio entre compras'])
plt.title('Distribuição dos Intervalos Médios entre Compras')
plt.ylabel('Intervalo Médio (dias)')
plt.show()

"""### Observações Gerais
Basicamente, a máquina de venda de café é uma venda de oportunidade. Temos uma quantidade muito pequena de clientes fidelizados (10 clientes fidelizados em mais de 1200 clientes cadastrados) que mostra que a grande maioria das vendas acontecem por clientes que estão passando por ali pontualmente e compram muito pontualmente. No caso desses alguns poucos fidelizados temos alguns que consomem com bastente frequencia.

A falta de fidelização é corroborada também pelo tempo médio desde a última compra e principalmente intervalo médio entre compras (que avalia 505 clientes que comprara mais de uma vez) com uma quantidade grande de outliers com muitos dias de diferença entre uma compra e outra.

O indicado para o negócio é criar ações para aumentar esse base fidelizada, convertendo esses clientes pontuais em fiéis utilizando estratégias de marketing.

## Tratamento de Valores Nulos

O dataset Coffee Sales possui 89 valores nulos na coluna card (que representa o tipo de cartão utilizado). Quando o é em dinheiro, essa área fica sem ser preenchida. Vamos ajustar para substituir esses valores nulos pela informação "cash" que vai sinalizar que foi um pagamento em dinheiro e por isso, não tem realmente um número de cartão associado.
"""

# Verificar a presença de valores nulos no dataset original
print("Valores nulos antes do tratamento:")
display(dataset.isnull().sum())

# Substituir os valores nulos na coluna 'card' por 'cash'
dataset['card'].fillna('cash', inplace=True)

"""# Pré-Processamento de Dados

O pré-processamento de dados é uma etapa crucial para preparar os dados para modelagem, garantindo que estejam no formato correto e otimizados para o desempenho do algoritmo.

## One-Hot Enconding

Agora, olhando para os produtos vamos usar a técnica para transformar coffee_name, variável categórica nominal em números binários, o que vai facilitar utilizarmos o modelo em alguns algoritmos de machine learning.
"""

# Aplicar One-Hot Encoding na coluna 'coffee_name'
coffee_name_encoded = pd.get_dummies(dataset['coffee_name'], prefix='coffee')

# Exibir as primeiras linhas do DataFrame codificado
print("Primeiras linhas do DataFrame com 'coffee_name' codificado:")
display(coffee_name_encoded.head())

# Exibir o formato do novo DataFrame para ver as novas colunas
print("\nFormato do DataFrame codificado:")
print(coffee_name_encoded.shape)

"""### Combinando dataframe codificado com o atual."""

# Combinar o DataFrame original com o DataFrame codificado
# Vamos remover a coluna 'coffee_name' original antes de concatenar para evitar redundância
dataset_combined = pd.concat([dataset.drop('coffee_name', axis=1), coffee_name_encoded], axis=1)

# Exibir as primeiras linhas do novo DataFrame combinado
print("Primeiras linhas do DataFrame combinado:")
display(dataset_combined.head())

# Opcional: Exibir o formato do DataFrame combinado
print("\nFormato do DataFrame combinado:")
print(dataset_combined.shape)

"""# Conclusão

Com base na análise exploratória e no pré-processamento do dataset Coffee Sales, podemos tirar várias conclusões importantes sobre o comportamento das vendas e dos clientes:

1.  **Tipos de Café Mais Vendidos:** A análise da distribuição de vendas por tipo de café mostrou claramente quais produtos são os "carros-chefes", com uma concentração significativa das vendas em poucos itens (como Americano with Milk e Latte), representando uma grande porcentagem do total de vendas. Isso sugere que focar na disponibilidade e promoção desses produtos é crucial.

2.  **Distribuição de Vendas por Hora:** O histograma das vendas por hora revelou os períodos de maior movimento ao longo do dia. Identificamos que as vendas ocorrem majoritariamente durante o horário comercial, com alguns picos, mas geralmente uma distribuição relativamente estável durante grande parte do dia, com quedas notáveis em certas horas, possivelmente relacionadas a pausas ou reabastecimento.

3.  **Formas de Pagamento:** A análise das formas de pagamento confirmou a forte preferência dos clientes pelo pagamento via cartão, com o pagamento em dinheiro representando uma parcela muito menor das transações. Isso é uma informação valiosa para a gestão dos métodos de pagamento.

4.  **Análise de Clientes e Fidelização:** A análise das métricas de cliente (contagem de transações, ticket médio, tempo desde a última compra e intervalo médio entre compras) sugere que a grande maioria das vendas provém de clientes com baixa frequência de compra. Um número muito pequeno de clientes se encaixa no critério de "fidelizados" com base na frequência definida. O gráfico de dispersão e a análise do intervalo médio entre compras reforçam a ideia de que a máquina atende principalmente a vendas de oportunidade, com a maioria dos clientes comprando poucas vezes e com longos intervalos entre as compras para aqueles que retornam.

5.  **Recomendações para o Negócio:** Dada a baixa taxa de fidelização, uma recomendação chave seria desenvolver estratégias para aumentar a retenção de clientes e incentivar compras repetidas. Isso poderia incluir programas de fidelidade baseados em cartão, promoções para clientes recorrentes, ou melhorias na experiência de compra para converter clientes pontuais em habituais.

Em resumo, o dataset forneceu insights valiosos sobre o desempenho dos produtos, padrões de vendas ao longo do dia, preferências de pagamento e, crucialmente, a natureza majoritariamente transacional da base de clientes, destacando a necessidade de estratégias focadas em fidelização.
"""